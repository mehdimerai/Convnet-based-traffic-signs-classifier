{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic Sign Recognition Classifier\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "## Self-Driving Car Engineering project 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0: Load The Data\n",
    "\n",
    "After downloading the training set, unzip the compressed file and move test.p and train.p in the main directory of your project.\n",
    "The training set contains more then 50000 images labelled through more then 40 classes. The images was resized 32x32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load pickled data\n",
    "import pickle\n",
    "\n",
    "#You can change the path if you prefer change the training set files in another directory\n",
    "training_file = 'train.p'\n",
    "testing_file = 'test.p'\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_test, y_test = test['features'], test['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Dataset Summary & Exploration\n",
    "\n",
    "The pickled data is a dictionary with 4 key/value pairs:\n",
    "\n",
    "- `'features'` is a 4D array containing raw pixel data of the traffic sign images, (num examples, width, height, channels).\n",
    "- `'labels'` is a 2D array containing the label/class id of the traffic sign. The file `signnames.csv` contains id -> name mappings for each id.\n",
    "- `'sizes'` is a list containing tuples, (width, height) representing the the original width and height the image.\n",
    "- `'coords'` is a list containing tuples, (x1, y1, x2, y2) representing coordinates of a bounding box around the sign in the image. \n",
    "\n",
    "### Stats about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Number of training examples\n",
    "n_train = len(X_train)\n",
    "\n",
    "# Number of testing examples.\n",
    "n_test = len(y_train)\n",
    "\n",
    "# The shape of an traffic sign image\n",
    "image_shape = X_train[0].shape\n",
    "\n",
    "# Number of classes\n",
    "n_classes = 43\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display images from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Data exploration visualization goes here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n",
    "# I created a code that search and display traffic signs of each <<distinct>> class\n",
    "# Display viz could be improved\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "sign_list = []\n",
    "while len(sign_list) < 43:\n",
    "    index = random.randint(0, len(X_train))\n",
    "    if y_train[index] not in sign_list:\n",
    "        sign_list.append(y_train[index])\n",
    "        image = X_train[index].squeeze()\n",
    "        plt.figure(figsize=(1,1))\n",
    "        plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Step 2: Design and Test a Model Architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#The data was preprocessed using sklearn.utils.shuffle that proceeds to a random\n",
    "#permutation of X_train and y_train elements. By randomising the order of the training set, \n",
    "#this process keeps us from using an ordered training set that could affect our algorithm learning.\n",
    "from sklearn.utils import shuffle\n",
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### We split the data into training/validation/testing sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train,y_train, test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    " \n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 200\n",
    "\n",
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "def LeNet(x):    \n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    \n",
    "#Part 1\n",
    "    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 3, 6), mean = mu, stddev = sigma))\n",
    "    conv1_b = tf.Variable(tf.zeros(6))\n",
    "    conv1   = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    \n",
    "#Part 2\n",
    "    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean = mu, stddev = sigma))\n",
    "    conv2_b = tf.Variable(tf.zeros(16))\n",
    "    conv2   = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "#Part 3\n",
    "    fc0   = flatten(conv2)\n",
    "    \n",
    "#Part 4\n",
    "    fc1_W = tf.Variable(tf.truncated_normal(shape=(400, 120), mean = mu, stddev = sigma))\n",
    "    fc1_b = tf.Variable(tf.zeros(120))\n",
    "    fc1   = tf.matmul(fc0, fc1_W) + fc1_b\n",
    "    fc1    = tf.nn.relu(fc1)\n",
    "\n",
    "#Part 5\n",
    "    fc2_W  = tf.Variable(tf.truncated_normal(shape=(120, 84), mean = mu, stddev = sigma))\n",
    "    fc2_b  = tf.Variable(tf.zeros(84))\n",
    "    fc2    = tf.matmul(fc1, fc2_W) + fc2_b\n",
    "    fc2    = tf.nn.relu(fc2)\n",
    "\n",
    "#Part 6\n",
    "    fc3_W  = tf.Variable(tf.truncated_normal(shape=(84, 43), mean = mu, stddev = sigma))\n",
    "    fc3_b  = tf.Variable(tf.zeros(43))\n",
    "    logits = tf.matmul(fc2, fc3_W) + fc3_b\n",
    "    \n",
    "    return logits\n",
    "\n",
    "\n",
    "# x is a placeholder to store input batchs. init to None to accept any batc in any size\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "\n",
    "# labels are'nt one hot encoded yet !\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "\n",
    "# One hot encode the labels\n",
    "one_hot_y = tf.one_hot(y, 43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Architecture details:**\n",
    "\n",
    "**Part 1:**\n",
    "As suggested, we used LeNet architecture in order to implement our convolutional neural network. We started by building our first convolutional layer that output into 5X5X3 dimension filters with a depth of 6. In this cas we used a stride of 1 and a padding equal to 0 (refers to padding = padding='VALID'). Then we used a ReLU activation fonction where its output will be propagated to a pooling function (Max Pooling in our case) the Convolution layer output using 2x2 Kernal and 2x2 strides.\n",
    "\n",
    "**Part 2:**\n",
    "This the block 2, we continue the pipeline with a second convolutional layer that will proceed in the same way as the 1st one. It will use the 1st layer output as a parameter that will be multiplied by the second layer weights in order to calculate the output height and width. Like the 1st layer, we used a ReLU activation function and a Max Pooling function.\n",
    "\n",
    "**Part 3:**\n",
    "We make the output of the previous pipeline flat. This mean that we transform the 3D input data to an 1 dimension vector.\n",
    "\n",
    "**Part 4:**\n",
    "The previous pipeline output was propagated into a the 3rd layer which is fully Connected. At that level we multiply the received inputs by the weights and we adds the bias. Then we apply a ReLU activation function.\n",
    "\n",
    "**Part 5:**\n",
    "Same process than the previous bloc. This layer will receive even less input features.\n",
    "\n",
    "**Part 6:**\n",
    "Same thing in the bloc 6. We end the whole process by obtaining the logits that will demonstrates the predicted label performed by our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Hyperparameter \"rate\"\n",
    "rate = 0.001\n",
    "\n",
    "logits = LeNet(x)\n",
    "\n",
    "#Part 1\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits, one_hot_y)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss_operation)\n",
    "\n",
    "\n",
    "#Part 2\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    #Batch the dataset and run it through the evaluation pipeline\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        #Average the accuracy of each batch\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples\n",
    "\n",
    "\n",
    "#Training execution\n",
    "with tf.Session() as sess:\n",
    "    #We initialize the variables\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        #in each epoch we shuffle our training data\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        \n",
    "        #Put the training data by batch\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            #Train the model in each batch\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        \n",
    "        #Each epoch we evaluate the model\n",
    "        validation_accuracy = evaluate(X_validation, y_validation)\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        print()\n",
    "    #In the end we save the model    \n",
    "    saver.save(sess, 'lenet')\n",
    "    print(\"Model saved\")\n",
    "\n",
    "    \n",
    "#Evaluate the model on a Test dataset\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "\n",
    "    test_accuracy = evaluate(X_test, y_test)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We select new images (non included in the training or the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Load the images and plot them here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "from os import listdir\n",
    "from PIL import Image as PImage                                                            \n",
    "import numpy                                                                     \n",
    "import matplotlib.pyplot as plt                                                  \n",
    "import glob\n",
    "\n",
    "%matplotlib inline\n",
    "imageFolderPath = 'img/'\n",
    "im_array = np.array([])\n",
    "imagesList = []\n",
    "\n",
    "imagesList = listdir(imageFolderPath)\n",
    "nbr_examples = len(imagesList)\n",
    "\n",
    "print('here is a list of new images:')\n",
    "i=0\n",
    "while i<len(imagesList):\n",
    "    im = PImage.open(imageFolderPath+imagesList[i])\n",
    "    plt.figure(figsize=(1,1))\n",
    "    plt.imshow(im)\n",
    "    im_array = numpy.array([numpy.array(PImage.open(imageFolderPath+imagesList[i]))])\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We use the model to classify the new images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "    #print (\"Model restored.\")\n",
    "\n",
    "    #Test for few samples:\n",
    "    print('Let us see how much our model is able to classify them correctly:')\n",
    "    ex1 = numpy.array([numpy.array(PImage.open(imageFolderPath+imagesList[1]))])\n",
    "    ex2 = numpy.array([numpy.array(PImage.open(imageFolderPath+imagesList[6]))])\n",
    "    ex3 = numpy.array([numpy.array(PImage.open(imageFolderPath+imagesList[4]))])\n",
    "    ex4 = numpy.array([numpy.array(PImage.open(imageFolderPath+imagesList[2]))])\n",
    "    ex5 = numpy.array([numpy.array(PImage.open(imageFolderPath+imagesList[3]))])\n",
    "    \n",
    "    ex1 = ex1.astype(numpy.float32, copy=False)\n",
    "    ex2 = ex2.astype(numpy.float32, copy=False)\n",
    "    ex3 = ex3.astype(numpy.float32, copy=False)\n",
    "    ex4 = ex4.astype(numpy.float32, copy=False)\n",
    "    ex5 = ex5.astype(numpy.float32, copy=False)\n",
    "    \n",
    "    test_accuracy1 = evaluate(ex1, y_test)\n",
    "    print(\"Test Accuracy for the example 1= {:.3f}\".format(test_accuracy1))\n",
    "    test_accuracy2 = evaluate(ex2, y_test)\n",
    "    print(\"Test Accuracy for the example 2= {:.3f}\".format(test_accuracy2))\n",
    "    test_accuracy3 = evaluate(ex3, y_test)\n",
    "    print(\"Test Accuracy for the example 3= {:.3f}\".format(test_accuracy3))\n",
    "    test_accuracy4 = evaluate(ex4, y_test)\n",
    "    print(\"Test Accuracy for the example 4= {:.3f}\".format(test_accuracy4))\n",
    "    test_accuracy5 = evaluate(ex5, y_test)\n",
    "    print(\"Test Accuracy for the example 5= {:.3f}\".format(test_accuracy5))\n",
    "    test_accuracy5 = evaluate(ex5, y_test)\n",
    "    \n",
    "    \n",
    "    avr_accuracy_new = (test_accuracy1 + test_accuracy2 + test_accuracy3 + test_accuracy4 + test_accuracy5) / 5\n",
    "    diva_accuracy = test_accuracy-avr_accuracy_new\n",
    "    \n",
    "    print(\"Accuracy for the 5 images= {:.3f}\".format(average_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We Visualize the softmax probabilities here.\n",
    "\n",
    "numpy.set_printoptions(threshold=numpy.nan)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    a = tf.constant(ex3)\n",
    "    b = LeNet(ex3)\n",
    "    print(sess.run(tf.nn.softmax(a)))\n",
    "    print(sess.run(tf.nn.top_k(a, k=3)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
